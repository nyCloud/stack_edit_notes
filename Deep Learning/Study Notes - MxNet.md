# MxNet Notes
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

## 1. NDArray
### 1.1 Basic Operations
```python
## Create NDArray

# consecutive integers
x = mx.nd.arange(12)

# Get shape  & size
print(x.shape)
print(x.size)

# Reshape
x.reshape((3,4))
# Support auto infer
x.reshape((3,-1)) 

# Ones, zeros, eye
x = mx.nd.eye(3)
x = mx.nd.zeros((3, 3))
x = mx.nd.ones((3, 3))

## Matrix operations

# Element-wise
# + - * / **
# e^x => x.exp()

# Transpose 
x = x.T

# Matmul
mul = mx.nd.dot(x, y)

# Sum & norm * mean
s = x.sum()
n = x.norm().asscalar()
m = x.mean().asscalar()

# Concat
# Vertical
mx.nd.concat(x, x, x, dim = 0)
# Hori
mx.nd.concat(x, x, dim = 1)

# Inplace operation
x[:] = x + y

# npArray <=> NDArray
numpy_array = np.ones((3, 3))
mx_array = mx.nd.array(numpy_array)
numpy_array = mx_array.asnumpy()

```

### 1.2. Automatic Differentiation

#### Basic Example
Here is a simple example to compute gradient to a column vector.

$$ y = x x^T$$
$$ \frac{\partial y}{\partial x} = 2x $$

```python
x = mx.nd.arange(4)
x.attach_grad()
with autograd.record():
	y = nd.dot(x, x)
y.backwardx()
print(x.grad)
# result = [0, 2, 4, 6]
```
Please note that when y is not a scalar, MxNet will sum the elements in y to get a new variable y by default, and then find analytical gradient of the new y wrt x.

#### Detach Function
Detach function will create a new constant and detach its former computation steps.
```python
x = nd.arange(4)
x.attach_grad()
with autograd.record():
    y = x * x
    z = y * x
z.backward()
print(x.grad)
# 3x^2 = [ 0.  3. 12. 27.]

x = nd.arange(4)
x.attach_grad()
with autograd.record():
    y = x * x
    u = y.detach()
    z = u * x
z.backward()
print(x.grad)
# dz/dx = u = x^2 = [0. 1. 4. 9.]
```

Besides, when calling function u.attach_grad(), u = u.detach() will be implicitly called. 
```python
y = nd.ones(4) * 2 
y.attach_grad() 
with autograd.record():
	u = x * y 
	u.attach_grad() 
	z = u + x 
z.backward()
print(x.grad, u.grad, y.grad)
``` 

when executing inside autograd.record() mode, autograd.is_training() will return True, else False.

## 2. Deep Learning Computation
### 2.1. Block

As network complexity increases, we move from designing single to entire layers of neurons.

Neural network designs like ResNet-152 have a fair degree of regularity. They consist of blocks of repeated (or at least similarly designed) layers; these blocks then form the basis of more complex network designs.

__Customized Block__

```python
from mxnet import nd
from mxnet.gluon import nn


class MLP(nn.Block):

	def __init__(self, **kwargs):
		super(MLP, self).__init__(**kwargs)
		self.hidden = nn.Dense(256, activation='relu')
		self.output = nn.Dense(10)
		
	def forward(self, x):
		return self.output(self.hidden(x))
```

The _backward_ and _initialize_ methods will be automatically generated by MxNet.

Simple Usage
```python
net = MLP()
net.initialize()
net(x)
```

__Concat Blocks__
Sequential Blocks
```python
net =nn.Sequential()
net.add(blk_0)
net.add(blk_1)
net.add(blk_2)
...
net.initialize()
net(x)
```

__Create Constant Variable in Block__
```python
self.rand_weight = self.params.get_constant(
'rand_weight', nd.random.uniform(shape=(20, 20)))
```

## 3. Linear Neural Networks

### 3.1. Linear Regression

Let's get start with Data Loaders, An example of simple data iterator:
```python
def data_iter(batch_size, features, labels):
	
	num_examples = len(features)
	indices = list(range(num_examples))  
	random.shuffle(indices) 
	
	for i in range(0, num_examples, batch_size):
		j = nd.array(indices[i: min(i + batch_size, num_examples)]) 
		yield features.take(j), labels.take(j) 
		# The “take” function will then return the corresponding element 
		# based on the indices
```

Or we can also use the DataLoader provided by MxNet
```python
def load_array(data_arrays, batch_size, is_train=True):

	"""Construct a Gluon data loader""" 
	dataset = gluon.data.ArrayDataset(*data_arrays) 
	return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train) 

batch_size = 10 
data_iter = load_array((features, labels), batch_size)
```

Linear Regression Scratch

```python
w = nd.random.normal(scale=0.01, shape=(2, 1)) 
b = nd.zeros(shape=(1,))
w.attach_grad() 
b.attach_grad()

def linreg(X, w, b): 
	return nd.dot(X, w) + b
	
def squared_loss(y_hat, y): 
	return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2

def sgd(params, lr, batch_size): 
	for param in params: 
		param[:] = param - lr * param.grad / batch_size

lr = 0.03 
num_epochs = 3 
net = linreg 
loss = squared_loss 

for epoch in range(num_epochs):
	for X, y in data_iter(batch_size, features, labels): 
		with autograd.record(): 
			l = loss(net(X, w, b), y)
		l.backward()
		sgd([w, b], lr, batch_size) 
```
- Please note that when y is not a scalar, MxNet will sum the elements in y to get a new variable y by default, and then find analytical gradient of the new y wrt x. And that is why during SGD update, gradient need to be divided by batch_size.

Linear Regression in MxNet style
 
```python
from mxnet import init
from mxnet.gluon import nn
from mxnet.gluon import loss

# Create sequential network
net =nn.Sequential()

# Add single dense layer with single output
# In MxNet, input size will be automatically infered
net.add(nn.Dense(1))

# Initialize layer
net.initialize(init.Normal(bias=0, sigma=0.01))

# Define loss function
l = loss.L2Loss()

# Indicate Optimizer
t_coef = {'learning_rate': 0.03}
trainer = gluon.Trainer(net.collect_params(), 'sgd', t_coef)

# Train the Model
num_epochs = 3 
for epoch in range(1, num_epochs + 1): 
	
	for X, y in data_iter: 
		with autograd.record(): 
			l = loss(net(X), y) 
		l.backward() 
		trainer.step(batch_size) 
	
	l = loss(net(features), labels) 
	print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))
```

### 3.2. Linear Classification

__Softmax Function__
To define the model of linear classification, we will need Softmax Function:
$$ \hat{y} = softmax(o) $$
where
$$p(y|x)=\hat{y}_i = \frac{exp(o_i)}{\sum_j exp(o_j)}$$
most likely class
$$\hat{c}(o)=\underset{i}{\operatorname{argmax}} (\hat{y}_i)$$

And the prediction model is defined as
$$ \hat{y}_i = softmax(Wx_i + b)$$


In MxNet, we can easily use its implementation of softmax
```python
from mxnet import nd
x = nd.array([1,2,3,4])
y = nd.softmax(x)
# y = [0.0320586  0.08714432 0.23688284 0.6439143 ]
```

__Cross Entropy Loss__
We define the log-likelihood loss function as:
$$ l = -log\;p(y|x) = -\sum_jy_jlog(\hat{y}_j)$$
Note that yj is in one hot representation, thus this term works as a selector.

Now consider the case where we don’t just observe a single outcome but maybe, an entire distribution over outcomes. We can use the same representation as before for y. The only difference is that rather than a vector containing only binary entries, say (0, 0, 1), we now have a generic probability vector, say (0.1, 0.2, 0.7).

__@ Information Theory Basis__
- Entropy: how many bits of information (or randomness) are contained in data.
$$H[p] = \sum_j-p(j)log\:p(j)$$
- Cross Entropy:   the cross entropy between two probability distributions p and q  measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.
$$H[p, q] = \sum_j-p(j)log \:q(j)$$
- KL Divergence: one way to measure the difference between two distributions.
$$ D(p||q) = -\sum_jp(j)log\:q(j) - H[p] = \sum_jp(j)log\frac{p(j)}{q(j)}$$

__Scratch Version__
```python
import d2l 
from mxnet import autograd, nd, gluon

batch_size = 256 
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

num_inputs = 784 
num_outputs = 10 
W = nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs)) 
b = nd.zeros(num_outputs)
W.attach_grad() 
b.attach_grad()

def softmax(X): 
	X_exp = X.exp() 
	partition = X_exp.sum(axis=1, keepdims=True) 
	return X_exp / partition 
	# The broadcast mechanism is applied here

def net(X): 
	return softmax(nd.dot(X.reshape((-1, num_inputs)), W) + b)
	
def cross_entropy(y_hat, y): 
	# np.pick picks value from y_hat wrt one hot vec y
	return - nd.pick(y_hat, y).log()
```

The training process
```python
def train_epoch(net, train_iter, loss, updater): 
	metric = Accumulator(3) 

	if isinstance(updater, gluon.Trainer): 
		updater = updater.step 
	
	for x, y in train_iter:
		with autograd.record(): 
			y_hat = net(x) 
			l = loss(y_hat, y) 
		l.backward() 
		updater(x.shape[0]) 
		metric.add(l.sum().asscalar(), accuracy(y_hat, y), y.size)
		
	return metric[0]/metric[2], metric[1]/metric[2]
```
In order to animate the training process, we can use an Animator here.
```python
class Animator(object): 

	def __init__(self, xlabel=None, ylabel=None, legend=[], xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=None, nrows=1, ncols=1, figsize=(3.5, 2.5)): 
		"""Incrementally plot multiple lines."""
		d2l.use_svg_display() 
		self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize) 
		if nrows * ncols == 1: self.axes = [self.axes,]
		self.config_axes = lambda : d2l.set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend) 
		self.X, self.Y, self.fmts = None, None, fmts 

	def add(self, x, y):
		"""Add multiple data points into the figure.""" 
		if not hasattr(y, "__len__"):y = [y] 
		n = len(y) 
		if not hasattr(x, "__len__"): x = [x] * n 
		if not self.X: self.X = [[] for _ in range(n)] 
		if not self.Y: self.Y = [[] for _ in range(n)] 
		if not self.fmts: self.fmts = ['-'] * n 
	
		for i, (a, b) in enumerate(zip(x, y)): 
			if a is not None and b is not None: 
				self.X[i].append(a) 
				self.Y[i].append(b) 
		self.axes[0].cla() 

		for x, y, fmt in zip(self.X, self.Y, self.fmts): 
			self.axes[0].plot(x, y, fmt) 
		self.config_axes() 
		display.display(self.fig) 
		display.clear_output(wait=True)
```
Train function, please note we will repeatedly use this function later for simplicity. 
```python
def train(net, train_iter, test_iter, loss, num_epochs, updater): 
	trains, test_accs = [], [] 
	animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9], 
						legend=['train loss', 'train acc', 'test acc']) 
	
	for epoch in range(num_epochs): 
		train_metrics = train_epoch(net, train_iter, loss, updater) 
		test_acc = evaluate_accuracy(net, test_iter) 
		animator.add(epoch+1, train_metrics+(test_acc,))
```

__More MxNet Style__
```python
import d2l from mxnet import gluon, init from mxnet.gluon import nn

batch_size = 256 
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

net = nn.Sequential() net.add(nn.Dense(10)) net.initialize(init.Normal(sigma=0.01))
loss = gluon.loss.SoftmaxCrossEntropyLoss()
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})

num_epochs = 10 
train(net, train_iter, test_iter, loss, num_epochs, trainer)
```

- Due to the limitation of floating numbers, exp and log operation may easily get NaN output. Thus it would be smarter to combine Softmax and Cross Entropy steps during training process


## 4. Multi-Layer Perceptrons
__Activation Functions__
In order to introduce no-linearity into our deep model, we usually add a none-linear activation function onto layers.

Common activation functions includes:

- ReLU (Rectified Linear Unit)
$$ReLU(x) = max(x, 0)$$

- pReLU (Parameterized ReLU)
$$pReLU(x) = max(0, x) + \alpha\: min(0, x) $$

- Sigmoid Function
$$ sigmoid(x) = \frac{1} {1+exp(-x)} $$
$$ \frac{d\:sigmoid(x)}{dx} = sigmoid(x) (1-sigmoid(x)) $$

- Tanh Function
$$ tanh(x) = \frac {1-exp(-2x)}{1+exp(-2x)} $$

Since ReLU is used so commonly, ND array support s the ReLu function as a basic native operator.
```python
x = nd.arange(-8.0, 8.0, 0.1) x.attach_grad() 
with autograd.record(): 
	y = x.relu()
```

__scratch version__
```python
# Initalize Params
num_inputs, num_outputs, num_hiddens = 784, 10, 256 
w1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens))
b1 = nd.zeros(num_hiddens) 
w2 = nd.random.normal(scale=0.01, shape=(num_hiddens, num_outputs)) 
b2 = nd.zeros(num_outputs) 
params = [w1, b1, w2, b2] 
for param in params: 
	param.attach_grad()

def relu(x):
	return nd.maximum(x, 0)

def net(x):
	x = x.reshape((-1, num_inputs))
	h = relu(nd.dot(x, w1) + b1)
	return nd.dot(h, w2) + b2

loss = gluon.loss.SoftmaxCrossEntropyLoss()
num_epochs, lr = 10, 0.5 
train(net, train_iter, test_iter, loss, num_epochs, lambda batch_size: sgd(params, lr, batch_size))

```
__simplified version__
```python
import d2l 
from mxnet import gluon, init 
from mxnet.gluon import nn

net = nn.Sequential() 
net.add(nn.Dense(256, activation='relu'),
        nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))

batch_size, num_epochs = 256, 10 
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) 
loss = gluon.loss.SoftmaxCrossEntropyLoss() 
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5}) 
train(net, train_iter, test_iter, loss, num_epochs, trainer)
```

p141 ==> p175
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTUxMDc5ODYxOSwxODcxNzIxMTA3LC0xMz
cwMDM0MCw2OTc2MzM2MzAsLTQ1MjM5MDgxMSw1MzI5MDMzNDAs
LTE5MTczMTM4NDQsMTc0MTYwNDgyMCwtMTk2ODkwOTYxMCwtMT
E4NzIyMTczOCwxMDU5OTA3NjMsLTEzMDIxOTYwMDAsMzI1MDEz
ODg0LDEyMzM5NTcxOTQsMTM4NzQ2MDc1LC05ODQ0NDY0OTQsLT
E3Njg0MDAzNzIsLTExMTI5MjI1NTQsLTQ5NzI2NzY2MiwtMTky
MjQ0NzkzMl19
-->