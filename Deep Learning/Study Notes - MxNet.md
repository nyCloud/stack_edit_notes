# MxNet Notes
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

## 1. NDArray
### 1.1 Basic Operations
```python
## Create NDArray

# consecutive integers
x = mx.nd.arange(12)

# Get shape  & size
print(x.shape)
print(x.size)

# Reshape
x.reshape((3,4))
# Support auto infer
x.reshape((3,-1)) 

# Ones, zeros, eye
x = mx.nd.eye(3)
x = mx.nd.zeros((3, 3))
x = mx.nd.ones((3, 3))

## Matrix operations

# Element-wise
# + - * / **
# e^x => x.exp()

# Transpose 
x = x.T

# Matmul
mul = mx.nd.dot(x, y)

# Sum & norm * mean
s = x.sum()
n = x.norm().asscalar()
m = x.mean().asscalar()

# Concat
# Vertical
mx.nd.concat(x, x, x, dim = 0)
# Hori
mx.nd.concat(x, x, dim = 1)

# Inplace operation
x[:] = x + y

# npArray <=> NDArray
numpy_array = np.ones((3, 3))
mx_array = mx.nd.array(numpy_array)
numpy_array = mx_array.asnumpy()

```

### 1.2. Automatic Differentiation

#### Basic Example
Here is a simple example to compute gradient to a column vector.

$$ y = x x^T$$
$$ \frac{\partial y}{\partial x} = 2x $$

```python
x = mx.nd.arange(4)
x.attach_grad()
with autograd.record():
	y = nd.dot(x, x)
y.backwardx()
print(x.grad)
# result = [0, 2, 4, 6]
```
Please note that when y is not a scalar, MxNet will sum the elements in y to get a new variable y by default, and then find analytical gradient of the new y wrt x.

#### Detach Function
Detach function will create a new constant and detach its former computation steps.
```python
x = nd.arange(4)
x.attach_grad()
with autograd.record():
    y = x * x
    z = y * x
z.backward()
print(x.grad)
# 3x^2 = [ 0.  3. 12. 27.]

x = nd.arange(4)
x.attach_grad()
with autograd.record():
    y = x * x
    u = y.detach()
    z = u * x
z.backward()
print(x.grad)
# dz/dx = u = x^2 = [0. 1. 4. 9.]
```

Besides, when calling function u.attach_grad(), u = u.detach() will be implicitly called. 
```python
y = nd.ones(4) * 2 
y.attach_grad() 
with autograd.record():
	u = x * y 
	u.attach_grad() 
	z = u + x 
z.backward()
print(x.grad, u.grad, y.grad)
``` 

when executing inside autograd.record() mode, autograd.is_training() will return True, else False.


## 2. Deep Learning Computation

### 2.1. Block

As network complexity increases, we move from designing single to entire layers of neurons.

Neural network designs like ResNet-152 have a fair degree of regularity. They consist of blocks of repeated (or at least similarly designed) layers; these blocks then form the basis of more complex network designs.

__Customized Block__

```python
from mxnet import nd
from mxnet.gluon import nn


class MLP(nn.Block):

	def __init__(self, **kwargs):
		super(MLP, self).__init__(**kwargs)
		self.hidden = nn.Dense(256, activation='relu')
		self.output = nn.Dense(10)
		
	def forward(self, x):
		return self.output(self.hidden(x))
```

The _backward_ and _initialize_ methods will be automatically generated by MxNet.

Simple Usage
```python
net = MLP()
net.initialize()
net(x)
```

__Concat Blocks__
Sequential Blocks
```python
net =nn.Sequential()
net.add(blk_0)
net.add(blk_1)
net.add(blk_2)
...
net.initialize()
net(x)
```

__Create Constant Variable in Block__
```python
self.rand_weight = self.params.get_constant(
'rand_weight', nd.random.uniform(shape=(20, 20)))
```

__Get All Params__
```python
# parameters only for the first layer
print(net[0].collect_params())
# parameters of the entire network
print(net.collect_params())
```

__Params Init__
```python
variables.
# force_reinit ensures that the variables are initialized again, regardless of
# whether they were already initialized previously
net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)

# All init to 1
net.initialize(init=init.Constant(1), force_reinit=True)
```
If you want different subblock has different initialize functions.
```python
net[1].initialize(init=init.Constant(42), force_reinit=True)
net[0].weight.initialize(init=init.Xavier(), force_reinit=True)
```

__Custom Init__
```python
class MyInit(init.Initializer):
	def _init_weight(self, name, data):
		print('Init', name, data.shape)
		data[:] = nd.random.uniform(low=-10, high=10, shape=data.shape)
		data *= data.abs() >= 5
net.initialize(init=MyInit(), force_reinit=True)
net[0].weight.data()[0]
```

__Tied Params__
```python
net = nn.Sequential()

shared = nn.Dense(8, activation='relu')
net.add(nn.Dense(8, activation='relu'), 
        shared,
        shared,
        nn.Dense(8, activation='relu', params=shared.params),
        nn.Dense(10))
net.initialize()
x = nd.random.uniform(shape=(2, 20))
net(x)

print(net[1].weight.data()[0] == net[2].weight.data()[0])
print(net[1].weight.data()[0] == net[3].weight.data()[0])

net[1].weight.data()[:]=nd.random.uniform(0,10,net[1].weight.data().shape)

print(net[1].weight.data()[0] == net[2].weight.data()[0])
print(net[1].weight.data()[0] == net[3].weight.data()[0])
```

### 2.2. Custom Layers
__Layer without Parameters__
```python
from mxnet import gluon, nd
from mxnet.gluon import nn

class CenteredLayer(nn.Block)：
	def __init__(self, **kwargs):
		super(CenteredLayer, self).__init__(**kwargs)

	def forward(self, x):
		return x - x.mean
```

__Layer with Parameters__
```python
class ReluDense(nn.Block):
	def __init__(self, units, in_units, **kwargs):
		super(ReluDense, self).__init__(*kwargs)
		self.weight = self.params.get('weight', shape=(in_units, units))
		self.bias = self.params.get('bias', shape=(units,))

	def forward(self, x):
		linear = nd.dot(x, self.weight.data()) + self.bias.data() 
		return nd.relu(linear)
```

### 2.3. File I/O
An example to save network params
- Saving
```python
class MLP(nn.Block): 
	def __init__(self, **kwargs): 
		super(MLP, self).__init__(**kwargs) 
		self.hidden = nn.Dense(256, activation='relu') 
		self.output = nn.Dense(10) 

	def forward(self, x): 
		return self.output(self.hidden(x)) 

net = MLP() 
net.initialize() 
x = nd.random.uniform(shape=(2, 20)) 
y = net(x)

# Save params
net.save_parameters('mlp.params')
```

- Loading
```python
net = MLP()
net.load_parameters('mlp.params')
```

## 3. Linear Neural Networks

### 3.1. Linear Regression

Let's get start with Data Loaders, An example of simple data iterator:
```python
def data_iter(batch_size, features, labels):
	
	num_examples = len(features)
	indices = list(range(num_examples))  
	random.shuffle(indices) 
	
	for i in range(0, num_examples, batch_size):
		j = nd.array(indices[i: min(i + batch_size, num_examples)]) 
		yield features.take(j), labels.take(j) 
		# The “take” function will then return the corresponding element 
		# based on the indices
```

Or we can also use the DataLoader provided by MxNet
```python
def load_array(data_arrays, batch_size, is_train=True):

	"""Construct a Gluon data loader""" 
	dataset = gluon.data.ArrayDataset(*data_arrays) 
	return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train) 

batch_size = 10 
data_iter = load_array((features, labels), batch_size)
```

Linear Regression Scratch

```python
w = nd.random.normal(scale=0.01, shape=(2, 1)) 
b = nd.zeros(shape=(1,))
w.attach_grad() 
b.attach_grad()

def linreg(X, w, b): 
	return nd.dot(X, w) + b
	
def squared_loss(y_hat, y): 
	return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2

def sgd(params, lr, batch_size): 
	for param in params: 
		param[:] = param - lr * param.grad / batch_size

lr = 0.03 
num_epochs = 3 
net = linreg 
loss = squared_loss 

for epoch in range(num_epochs):
	for X, y in data_iter(batch_size, features, labels): 
		with autograd.record(): 
			l = loss(net(X, w, b), y)
		l.backward()
		sgd([w, b], lr, batch_size) 
```
- Please note that when y is not a scalar, MxNet will sum the elements in y to get a new variable y by default, and then find analytical gradient of the new y wrt x. And that is why during SGD update, gradient need to be divided by batch_size.

Linear Regression in MxNet style
 
```python
from mxnet import init
from mxnet.gluon import nn
from mxnet.gluon import loss

# Create sequential network
net =nn.Sequential()

# Add single dense layer with single output
# In MxNet, input size will be automatically infered
net.add(nn.Dense(1))

# Initialize layer
net.initialize(init.Normal(bias=0, sigma=0.01))

# Define loss function
l = loss.L2Loss()

# Indicate Optimizer
t_coef = {'learning_rate': 0.03}
trainer = gluon.Trainer(net.collect_params(), 'sgd', t_coef)

# Train the Model
num_epochs = 3 
for epoch in range(1, num_epochs + 1): 
	
	for X, y in data_iter: 
		with autograd.record(): 
			l = loss(net(X), y) 
		l.backward() 
		trainer.step(batch_size) 
	
	l = loss(net(features), labels) 
	print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))
```

### 3.2. Linear Classification

__Softmax Function__
To define the model of linear classification, we will need Softmax Function:
$$ \hat{y} = softmax(o) $$
where
$$p(y|x)=\hat{y}_i = \frac{exp(o_i)}{\sum_j exp(o_j)}$$
most likely class
$$\hat{c}(o)=\underset{i}{\operatorname{argmax}} (\hat{y}_i)$$

And the prediction model is defined as
$$ \hat{y}_i = softmax(Wx_i + b)$$


In MxNet, we can easily use its implementation of softmax
```python
from mxnet import nd
x = nd.array([1,2,3,4])
y = nd.softmax(x)
# y = [0.0320586  0.08714432 0.23688284 0.6439143 ]
```

__Cross Entropy Loss__
We define the log-likelihood loss function as:
$$ l = -log\;p(y|x) = -\sum_jy_jlog(\hat{y}_j)$$
Note that yj is in one hot representation, thus this term works as a selector.

Now consider the case where we don’t just observe a single outcome but maybe, an entire distribution over outcomes. We can use the same representation as before for y. The only difference is that rather than a vector containing only binary entries, say (0, 0, 1), we now have a generic probability vector, say (0.1, 0.2, 0.7).

__@ Information Theory Basis__
- Entropy: how many bits of information (or randomness) are contained in data.
$$H[p] = \sum_j-p(j)log\:p(j)$$
- Cross Entropy:   the cross entropy between two probability distributions p and q  measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.
$$H[p, q] = \sum_j-p(j)log \:q(j)$$
- KL Divergence: one way to measure the difference between two distributions.
$$ D(p||q) = -\sum_jp(j)log\:q(j) - H[p] = \sum_jp(j)log\frac{p(j)}{q(j)}$$

__Scratch Version__
```python
import d2l 
from mxnet import autograd, nd, gluon

batch_size = 256 
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

num_inputs = 784 
num_outputs = 10 
W = nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs)) 
b = nd.zeros(num_outputs)
W.attach_grad() 
b.attach_grad()

def softmax(X): 
	X_exp = X.exp() 
	partition = X_exp.sum(axis=1, keepdims=True) 
	return X_exp / partition 
	# The broadcast mechanism is applied here

def net(X): 
	return softmax(nd.dot(X.reshape((-1, num_inputs)), W) + b)
	
def cross_entropy(y_hat, y): 
	# np.pick picks value from y_hat wrt one hot vec y
	return - nd.pick(y_hat, y).log()
```

The training process
```python
def train_epoch(net, train_iter, loss, updater): 
	metric = Accumulator(3) 

	if isinstance(updater, gluon.Trainer): 
		updater = updater.step 
	
	for x, y in train_iter:
		with autograd.record(): 
			y_hat = net(x) 
			l = loss(y_hat, y) 
		l.backward() 
		updater(x.shape[0]) 
		metric.add(l.sum().asscalar(), accuracy(y_hat, y), y.size)
		
	return metric[0]/metric[2], metric[1]/metric[2]
```
In order to animate the training process, we can use an Animator here.
```python
class Animator(object): 

	def __init__(self, xlabel=None, ylabel=None, legend=[], xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=None, nrows=1, ncols=1, figsize=(3.5, 2.5)): 
		"""Incrementally plot multiple lines."""
		d2l.use_svg_display() 
		self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize) 
		if nrows * ncols == 1: self.axes = [self.axes,]
		self.config_axes = lambda : d2l.set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend) 
		self.X, self.Y, self.fmts = None, None, fmts 

	def add(self, x, y):
		"""Add multiple data points into the figure.""" 
		if not hasattr(y, "__len__"):y = [y] 
		n = len(y) 
		if not hasattr(x, "__len__"): x = [x] * n 
		if not self.X: self.X = [[] for _ in range(n)] 
		if not self.Y: self.Y = [[] for _ in range(n)] 
		if not self.fmts: self.fmts = ['-'] * n 
	
		for i, (a, b) in enumerate(zip(x, y)): 
			if a is not None and b is not None: 
				self.X[i].append(a) 
				self.Y[i].append(b) 
		self.axes[0].cla() 

		for x, y, fmt in zip(self.X, self.Y, self.fmts): 
			self.axes[0].plot(x, y, fmt) 
		self.config_axes() 
		display.display(self.fig) 
		display.clear_output(wait=True)
```
Train function, please note we will repeatedly use this function later for simplicity. 
```python
def train(net, train_iter, test_iter, loss, num_epochs, updater): 
	trains, test_accs = [], [] 
	animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9], 
						legend=['train loss', 'train acc', 'test acc']) 
	
	for epoch in range(num_epochs): 
		train_metrics = train_epoch(net, train_iter, loss, updater) 
		test_acc = evaluate_accuracy(net, test_iter) 
		animator.add(epoch+1, train_metrics+(test_acc,))
```

__More MxNet Style__
```python
import d2l from mxnet import gluon, init from mxnet.gluon import nn

batch_size = 256 
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

net = nn.Sequential() net.add(nn.Dense(10)) net.initialize(init.Normal(sigma=0.01))
loss = gluon.loss.SoftmaxCrossEntropyLoss()
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})

num_epochs = 10 
train(net, train_iter, test_iter, loss, num_epochs, trainer)
```

- Due to the limitation of floating numbers, exp and log operation may easily get NaN output. Thus it would be smarter to combine Softmax and Cross Entropy steps during training process


## 4. Multi-Layer Perceptrons

### 4.1 Basic MLP

__Activation Functions__
In order to introduce no-linearity into our deep model, we usually add a none-linear activation function onto layers.

Common activation functions includes:

- ReLU (Rectified Linear Unit)
$$ReLU(x) = max(x, 0)$$

- pReLU (Parameterized ReLU)
$$pReLU(x) = max(0, x) + \alpha\: min(0, x) $$

- Sigmoid Function
$$ sigmoid(x) = \frac{1} {1+exp(-x)} $$
$$ \frac{d\:sigmoid(x)}{dx} = sigmoid(x) (1-sigmoid(x)) $$

- Tanh Function
$$ tanh(x) = \frac {1-exp(-2x)}{1+exp(-2x)} $$

Since ReLU is used so commonly, ND array support s the ReLu function as a basic native operator.
```python
x = nd.arange(-8.0, 8.0, 0.1) x.attach_grad() 
with autograd.record(): 
	y = x.relu()
```

__scratch version__
```python
# Initalize Params
num_inputs, num_outputs, num_hiddens = 784, 10, 256 
w1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens))
b1 = nd.zeros(num_hiddens) 
w2 = nd.random.normal(scale=0.01, shape=(num_hiddens, num_outputs)) 
b2 = nd.zeros(num_outputs) 
params = [w1, b1, w2, b2] 
for param in params: 
	param.attach_grad()

def relu(x):
	return nd.maximum(x, 0)

def net(x):
	x = x.reshape((-1, num_inputs))
	h = relu(nd.dot(x, w1) + b1)
	return nd.dot(h, w2) + b2

loss = gluon.loss.SoftmaxCrossEntropyLoss()
num_epochs, lr = 10, 0.5 
train(net, train_iter, test_iter, loss, num_epochs, lambda batch_size: sgd(params, lr, batch_size))

```
__simplified version__
```python
import d2l 
from mxnet import gluon, init 
from mxnet.gluon import nn

net = nn.Sequential() 
net.add(nn.Dense(256, activation='relu'),
        nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))

batch_size, num_epochs = 256, 10 
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) 
loss = gluon.loss.SoftmaxCrossEntropyLoss() 
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5}) 
train(net, train_iter, test_iter, loss, num_epochs, trainer)
```

#### 4.2 Model Selection, Underfitting and Overfitting

__Weight Decay__

Weight decay (commonly called L2 regularization), might be the most widely-used technique for regularizing parametric machine learning models. Considering a linear function $f(x)=w^Tx$, it would be simple if its weight vector is small. We can measure this via $||w||^2$ . One way of keeping the weight vector small is to add its norm as a penalty term to the problem of minimizing the loss. Thus we replace our original objective, minimize the prediction error on the training labels, with new objective, minimize the sum of the prediction error and the penalty term. Now, if the weight vector becomes too large, our learning algorithm will find more profit in minimizing the norm $||w||^2$ versus minimizing the training error. That’s exactly what we want. 

Following code block shows the basic usage of weight decay in MxNet.

```python
def train_gluon(wd):
	net = nn.Sequential()
	net.add(nn.Dense(1))
	net.initialize(init.Normal(sigma=1))
	
	loss = gluon.loss.L2Loss()
	num_epochs, lr = 100, 0.003
	
	# The weight parameter has been decayed. Weight names generally end with "weight".
	trainer_w = gluon.Trainer(net.collect_params('.*weight'), 'sgd',
                              {'learning_rate': lr, 'wd': wd})
	# The bias parameter has not decayed. Bias names generally end with "bias"
	trainer_b = gluon.Trainer(net.collect_params('.*bias'), 'sgd',
                              {'learning_rate': lr})
                              
	animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
	xlim=[1, num_epochs], legend=['train', 'test'])
	for epoch in range(1, num_epochs+1):
		for X, y in train_iter:
			with autograd.record():
				l = loss(net(X), y)
				l.backward()
				
		# Call the step function on each of the two Trainer instances to
		# update the weight and bias separately
		trainer_w.step(batch_size)
		trainer_b.step(batch_size)
```

p153

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE2OTUyMTM0NTcsMTY3MzkxNjgxNCwyMj
Y0MjcyMjQsLTIwMDEwNjExMDUsLTEwODYwODEzNzcsMTUyNTgx
MTE4NiwtMTk3MjE4MDM2LC0xNzYyMzUyOTUzLC0xNDI4NjEwNj
IzLC0xNDcyMTU3Nzk0LDEzMDc5ODY1ODEsLTQ1NTYwMDEwNywx
MzAxNjg3NzA3LC0xOTQ4NjY1OTEyLC0xOTQ0NjMxMDg1LDg4MD
M3MDAzOSwtNTEwNzk4NjE5LDE4NzE3MjExMDcsLTEzNzAwMzQw
LDY5NzYzMzYzMF19
-->